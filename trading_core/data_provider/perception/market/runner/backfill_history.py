import time
from trading_core.data_provider.perception.market.binance.binance_fetcher import (
    BinanceRawFetcher
)
from trading_core.data_provider.perception.market.storage.csv_market_writer import (
    MarketCSVWriter
)
def detect_kline_gaps(records: list[dict], interval: str) -> list[tuple[int, int]]:
    """
    åµæ¸¬ç¼ºå¤±çš„ K ç·šæ™‚é–“å€é–“
    å›å‚³ [(from_ts, to_ts), ...]ï¼ˆç§’ï¼‰
    """
    if not records:
        return []

    interval_sec_map = {
        "1m": 60,
        "5m": 300,
        "15m": 900,
        "30m": 1800,
        "1h": 3600,
        "4h": 14400,
        "1d": 86400,
    }

    if interval not in interval_sec_map:
        raise ValueError(f"Unsupported interval: {interval}")

    step = interval_sec_map[interval]

    records = sorted(records, key=lambda r: r["open_time"])

    gaps = []
    prev = None

    for r in records:
        if prev is not None:
            expected_open = prev["open_time"] + step
            if expected_open < r["open_time"]:
                gaps.append((expected_open, r["open_time"] - step))
        prev = r

    return gaps

def _normalize_record(r: dict) -> dict:
    """
    å°‡ Binance å„ç¨®å¯èƒ½çš„ kline schema
    çµ±ä¸€è½‰æˆ v1.7 æ¨™æº– schemaï¼ˆç§’ï¼‰
    """

    # open / close timeï¼ˆç§’ï¼‰
    if "open_time" in r:
        open_ts = int(float(r["open_time"]))
    elif "kline_open_ts" in r:
        open_ts = int(float(r["kline_open_ts"]))
    elif "open_time_ms" in r:
        open_ts = int(float(r["open_time_ms"]) / 1000)
    else:
        raise KeyError("open_time")

    if "close_time" in r:
        close_ts = int(float(r["close_time"]))
    elif "kline_close_ts" in r:
        close_ts = int(float(r["kline_close_ts"]))
    elif "close_time_ms" in r:
        close_ts = int(float(r["close_time_ms"]) / 1000)
    else:
        raise KeyError("close_time")

    return {
        "open_time": open_ts,     # â­ ç§’
        "close_time": close_ts,   # â­ ç§’
        "open": float(r["open"]),
        "high": float(r["high"]),
        "low": float(r["low"]),
        "close": float(r["close"]),
        "volume": float(r["volume"]),
    }

def backfill(
    symbol: str,
    interval: str,
    from_ts: int,
    to_ts: int,
    csv_root: str,
    provider=None,
):
    print(f"ğŸ”„ Backfill {symbol} {interval} from {from_ts} â†’ {to_ts}")

    fetcher = BinanceRawFetcher()
    writer = MarketCSVWriter(root=csv_root)

    # =====================================================
    # âœ… Step 0: è®€å–æ—¢æœ‰ CSVï¼ˆé—œéµï¼‰
    # =====================================================
    existing_records = []
    try:
        existing_raw = writer.read(symbol=symbol, interval=interval)
        for r in existing_raw:
            try:
                existing_records.append(_normalize_record(r))
            except Exception:
                pass
    except Exception:
        pass  # CSV ä¸å­˜åœ¨ä¹Ÿæ²’é—œä¿‚

    # =====================================================
    # âœ… Step 1: æª¢æŸ¥ CSV æ˜¯å¦æå£
    # =====================================================
    bad_start, bad_end = detect_corrupted_ranges(existing_records)

    if bad_start is not None:
        print(f"ğŸš¨ Corrupted CSV detected: {bad_start} â†’ {bad_end}")
        from_ts = bad_start  # â¬…ï¸ é—œéµï¼šæ•´æ®µé‡å»º

    # =====================================================
    # âœ… Step 2: æŠ“æ­·å²è³‡æ–™ï¼ˆçœŸå¯¦å¸‚å ´ï¼‰
    # =====================================================
    raw_records = fetcher.fetch_history(
        symbol=symbol,
        interval=interval,
        since_ts=from_ts,
        until_ts=to_ts,
    )

    fetched_records = []
    for r in raw_records:
        try:
            fetched_records.append(_normalize_record(r))
        except Exception as e:
            print(f"[Backfill] âš  normalize skip: {e}")

    # =====================================================
    # âœ… Step 3: åˆä½µ existing + fetched
    # =====================================================
    records = existing_records + fetched_records

    if not records:
        print("â„¹ï¸ No valid records")
        return

    # =====================================================
    # âœ… Step 4: åµæ¸¬æ•´é«”æ™‚é–“è»¸çš„ gap
    # =====================================================
    gaps = detect_kline_gaps(records, interval)

    # =====================================================
    # âœ… Step 5: ä¿®å¾© gapï¼ˆæŠ“çœŸå¯¦è³‡æ–™ï¼‰
    # =====================================================
    for gap_from, gap_to in gaps:
        print(f"ğŸ§© Repair gap {symbol} {interval}: {gap_from} â†’ {gap_to}")

        missing_raw = fetcher.fetch_history(
            symbol=symbol,
            interval=interval,
            since_ts=gap_from,
            until_ts=gap_to,
        )

        for r in missing_raw:
            try:
                records.append(_normalize_record(r))
            except Exception:
                pass

    # =====================================================
    # âœ… Step 6: æ’åº + å»é‡ï¼ˆæ™‚é–“è»¸å”¯ä¸€ï¼‰
    # =====================================================
    dedup = {}
    for r in records:
        dedup[r["open_time"]] = r

    records = sorted(dedup.values(), key=lambda r: r["open_time"])

    # =====================================================
    # âœ… Step 7: ä¸€æ¬¡æ€§å¯« CSV
    # =====================================================
    writer.write(records, symbol=symbol, interval=interval)

    # =====================================================
    # âœ… Step 8:ï¼ˆå¯é¸ï¼‰emit history
    # =====================================================
    if provider is not None:
        for r in records:
            try:
                provider.emit_kline(
                    symbol=symbol,
                    interval=interval,
                    open_time_ms=int(r["open_time"] * 1000),
                    close_time_ms=int(r["close_time"] * 1000),
                    open_price=r["open"],
                    high_price=r["high"],
                    low_price=r["low"],
                    close_price=r["close"],
                    volume=r["volume"],
                    source="history",
                )
            except Exception:
                pass

    print(f"âœ… Backfill done: {len(records)} records")
    time.sleep(1)


def detect_corrupted_ranges(records: list[dict]) -> tuple[int | None, int | None]:
    """
    å›å‚³ (bad_start_ts, bad_end_ts)
    """
    seen = {}
    bad_times = []

    for r in records:
        ot = r.get("open_time")

        # æ¬„ä½ä¸å®Œæ•´
        if any(r.get(k) is None for k in ["open", "high", "low", "close", "volume"]):
            bad_times.append(ot)
            continue

        # é‡è¤‡æ™‚é–“
        if ot in seen:
            bad_times.append(ot)
        else:
            seen[ot] = r

    if not bad_times:
        return None, None

    return min(bad_times), max(bad_times)
